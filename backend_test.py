import requests
import sys
import json
import time
import os
import tempfile
from datetime import datetime

class BilginAIAPITester:
    def __init__(self, base_url="https://rag-websearch.preview.emergentagent.com/api"):
        self.base_url = base_url
        self.tests_run = 0
        self.tests_passed = 0
        self.conversation_id = None
        self.hybrid_tests_passed = 0
        self.hybrid_tests_run = 0
        self.file_tests_passed = 0
        self.file_tests_run = 0

    def run_test(self, name, method, endpoint, expected_status, data=None):
        """Run a single API test"""
        url = f"{self.base_url}/{endpoint}" if endpoint else self.base_url
        headers = {'Content-Type': 'application/json'}

        self.tests_run += 1
        print(f"\nğŸ” Testing {name}...")
        print(f"   URL: {url}")
        
        try:
            if method == 'GET':
                response = requests.get(url, headers=headers, timeout=10)
            elif method == 'POST':
                response = requests.post(url, json=data, headers=headers, timeout=30)
            elif method == 'DELETE':
                response = requests.delete(url, headers=headers, timeout=10)

            print(f"   Status Code: {response.status_code}")
            
            success = response.status_code == expected_status
            if success:
                self.tests_passed += 1
                print(f"âœ… Passed - Status: {response.status_code}")
                try:
                    response_data = response.json()
                    print(f"   Response: {json.dumps(response_data, indent=2)[:200]}...")
                    return True, response_data
                except:
                    return True, {}
            else:
                print(f"âŒ Failed - Expected {expected_status}, got {response.status_code}")
                try:
                    error_data = response.json()
                    print(f"   Error: {error_data}")
                except:
                    print(f"   Error: {response.text}")
                return False, {}

        except Exception as e:
            print(f"âŒ Failed - Error: {str(e)}")
            return False, {}

    def test_root_endpoint(self):
        """Test root API endpoint"""
        success, response = self.run_test(
            "Root API Endpoint",
            "GET",
            "",
            200
        )
        return success

    def test_get_conversations_empty(self):
        """Test getting conversations when empty"""
        success, response = self.run_test(
            "Get Conversations (Empty)",
            "GET",
            "conversations",
            200
        )
        return success

    def test_create_conversation(self):
        """Test creating a new conversation"""
        success, response = self.run_test(
            "Create New Conversation",
            "POST",
            "conversations",
            200,
            data={"title": "Test Sohbet"}
        )
        if success and 'id' in response:
            self.conversation_id = response['id']
            print(f"   Created conversation ID: {self.conversation_id}")
        return success

    def test_get_conversations_with_data(self):
        """Test getting conversations after creating one"""
        success, response = self.run_test(
            "Get Conversations (With Data)",
            "GET",
            "conversations",
            200
        )
        if success and isinstance(response, list) and len(response) > 0:
            print(f"   Found {len(response)} conversations")
        return success

    def test_get_messages_empty(self):
        """Test getting messages from empty conversation"""
        if not self.conversation_id:
            print("âŒ Skipped - No conversation ID available")
            return False
            
        success, response = self.run_test(
            "Get Messages (Empty)",
            "GET",
            f"conversations/{self.conversation_id}/messages",
            200
        )
        return success

    def test_send_message(self):
        """Test sending a message and getting AI response"""
        if not self.conversation_id:
            print("âŒ Skipped - No conversation ID available")
            return False
            
        success, response = self.run_test(
            "Send Message (AI Integration Test)",
            "POST",
            f"conversations/{self.conversation_id}/messages",
            200,
            data={"content": "Merhaba, sen kimsin?", "mode": "chat"}
        )
        
        if success:
            print("   âœ… Message sent successfully")
            if 'content' in response:
                print(f"   AI Response: {response['content'][:100]}...")
            else:
                print("   âš ï¸  No AI response content found")
        
        return success

    def test_get_messages_with_data(self):
        """Test getting messages after sending one"""
        if not self.conversation_id:
            print("âŒ Skipped - No conversation ID available")
            return False
            
        success, response = self.run_test(
            "Get Messages (With Data)",
            "GET",
            f"conversations/{self.conversation_id}/messages",
            200
        )
        
        if success and isinstance(response, list):
            print(f"   Found {len(response)} messages")
            for i, msg in enumerate(response):
                print(f"   Message {i+1}: {msg.get('role', 'unknown')} - {msg.get('content', '')[:50]}...")
        
        return success

    def test_send_second_message(self):
        """Test sending a second message"""
        if not self.conversation_id:
            print("âŒ Skipped - No conversation ID available")
            return False
            
        success, response = self.run_test(
            "Send Second Message",
            "POST",
            f"conversations/{self.conversation_id}/messages",
            200,
            data={"content": "BugÃ¼n hava nasÄ±l?", "mode": "chat"}
        )
        return success

    def test_delete_conversation(self):
        """Test deleting a conversation"""
        if not self.conversation_id:
            print("âŒ Skipped - No conversation ID available")
            return False
            
        success, response = self.run_test(
            "Delete Conversation",
            "DELETE",
            f"conversations/{self.conversation_id}",
            200
        )
        return success

    def test_get_deleted_conversation(self):
        """Test that deleted conversation is gone"""
        if not self.conversation_id:
            print("âŒ Skipped - No conversation ID available")
            return False
            
        success, response = self.run_test(
            "Get Messages from Deleted Conversation",
            "GET",
            f"conversations/{self.conversation_id}/messages",
            200  # Should return empty array, not 404
        )
        return success

    def test_hybrid_system_casual_question(self):
        """Test Scenario 1: Casual Questions (AnythingLLM Only) - 'merhaba'"""
        print("\nğŸ§ª HYBRID SYSTEM TEST 1: Casual Questions (AnythingLLM Only)")
        
        # Create new conversation for hybrid tests
        success, response = self.run_test(
            "Create Conversation for Hybrid Test 1",
            "POST",
            "conversations",
            200,
            data={"title": "Hybrid Test - Casual"}
        )
        
        if not success:
            return False
            
        test_conv_id = response.get('id')
        if not test_conv_id:
            print("âŒ Failed to get conversation ID")
            return False
        
        # Test casual greeting
        start_time = time.time()
        success, response = self.run_test(
            "Send Casual Question: 'merhaba'",
            "POST",
            f"conversations/{test_conv_id}/messages",
            200,
            data={"content": "merhaba", "mode": "chat"}
        )
        
        response_time = time.time() - start_time
        
        if success:
            self.hybrid_tests_run += 1
            ai_response = response.get('content', '')
            print(f"   Response Time: {response_time:.2f} seconds")
            print(f"   AI Response: {ai_response[:150]}...")
            
            # Check if response is appropriate for casual greeting
            if any(word in ai_response.lower() for word in ['merhaba', 'selam', 'nasÄ±lsÄ±n', 'yardÄ±m']):
                print("âœ… PASSED: Appropriate casual response received")
                self.hybrid_tests_passed += 1
                
                # Check response time (should be fast for AnythingLLM only)
                if response_time < 10:
                    print("âœ… PASSED: Fast response time (AnythingLLM only)")
                else:
                    print(f"âš ï¸  WARNING: Slow response time ({response_time:.2f}s) - may indicate web search was used")
                    
            else:
                print("âŒ FAILED: Inappropriate response for casual greeting")
        
        return success

    def test_hybrid_system_math_question(self):
        """Test Scenario 4: Matematik (AnythingLLM gÃ¼Ã§lÃ¼) - '144 Ã· 12 kaÃ§ eder?'"""
        print("\nğŸ§ª HYBRID SYSTEM TEST 2: Math Questions (AnythingLLM Strong)")
        
        # Create new conversation
        success, response = self.run_test(
            "Create Conversation for Hybrid Test 2",
            "POST",
            "conversations",
            200,
            data={"title": "Hybrid Test - Math"}
        )
        
        if not success:
            return False
            
        test_conv_id = response.get('id')
        
        # Test math question - should use AnythingLLM, NOT web search
        start_time = time.time()
        success, response = self.run_test(
            "Send Math Question: '144 Ã· 12 kaÃ§ eder?'",
            "POST",
            f"conversations/{test_conv_id}/messages",
            200,
            data={"content": "144 Ã· 12 kaÃ§ eder?", "mode": "chat"}
        )
        
        response_time = time.time() - start_time
        
        if success:
            self.hybrid_tests_run += 1
            ai_response = response.get('content', '')
            print(f"   Response Time: {response_time:.2f} seconds")
            print(f"   AI Response: {ai_response[:150]}...")
            
            # Check if response contains correct answer (12)
            if '12' in ai_response:
                print("âœ… PASSED: Correct math answer (12) found in response")
                self.hybrid_tests_passed += 1
                
                # Check that no web search indicators are present (should use AnythingLLM only)
                web_indicators = ['web araÅŸtÄ±rmasÄ±', 'gÃ¼ncel web', 'kaynaklarÄ±ndan']
                if not any(indicator in ai_response.lower() for indicator in web_indicators):
                    print("âœ… PASSED: No web search indicators (AnythingLLM used as expected)")
                else:
                    print("âŒ FAILED: Web search indicators found - should use AnythingLLM only for math")
                    
            else:
                print("âŒ FAILED: Incorrect or missing math answer (should be 12)")
        
        return success

    def test_hybrid_system_weather_direct_web(self):
        """Test Scenario 2: Hava Durumu (Google'dan aratÄ±labilir) - 'Ä°stanbul hava durumu nasÄ±l?'"""
        print("\nğŸ§ª HYBRID SYSTEM TEST 3A: Weather (Direct Web Search)")
        
        # Create new conversation
        success, response = self.run_test(
            "Create Conversation for Weather Test",
            "POST",
            "conversations",
            200,
            data={"title": "Hybrid Test - Weather"}
        )
        
        if not success:
            return False
            
        test_conv_id = response.get('id')
        
        # Test weather question - should go directly to web search
        start_time = time.time()
        success, response = self.run_test(
            "Send Weather Question: 'Ä°stanbul hava durumu nasÄ±l?'",
            "POST",
            f"conversations/{test_conv_id}/messages",
            200,
            data={"content": "Ä°stanbul hava durumu nasÄ±l?", "mode": "chat"}
        )
        
        response_time = time.time() - start_time
        
        if success:
            self.hybrid_tests_run += 1
            ai_response = response.get('content', '')
            print(f"   Response Time: {response_time:.2f} seconds")
            print(f"   AI Response: {ai_response[:200]}...")
            
            # Check if web search was used (should go directly to web search, NOT AnythingLLM)
            web_indicators = ['web araÅŸtÄ±rmasÄ±', 'gÃ¼ncel', 'hava', 'sÄ±caklÄ±k', 'derece']
            has_web_indicators = any(indicator in ai_response.lower() for indicator in web_indicators)
            
            # Check if response contains weather information
            has_weather_info = any(pattern in ai_response.lower() for pattern in ['hava', 'sÄ±caklÄ±k', 'derece', 'yaÄŸmur', 'gÃ¼neÅŸ', 'bulut'])
            
            if has_web_indicators or has_weather_info:
                print("âœ… PASSED: Web search used directly for weather (bypassed AnythingLLM)")
                self.hybrid_tests_passed += 1
            else:
                print("âŒ FAILED: Should use web search directly for weather, not AnythingLLM")
        
        return success

    def test_hybrid_system_sports_direct_web(self):
        """Test Scenario 3: Spor Sonucu (Google'dan aratÄ±labilir) - 'Galatasaray son maÃ§ skoru nedir?'"""
        print("\nğŸ§ª HYBRID SYSTEM TEST 3B: Sports (Direct Web Search)")
        
        # Create new conversation
        success, response = self.run_test(
            "Create Conversation for Sports Test",
            "POST",
            "conversations",
            200,
            data={"title": "Hybrid Test - Sports"}
        )
        
        if not success:
            return False
            
        test_conv_id = response.get('id')
        
        # Test sports question - should go directly to web search
        start_time = time.time()
        success, response = self.run_test(
            "Send Sports Question: 'Galatasaray son maÃ§ skoru nedir?'",
            "POST",
            f"conversations/{test_conv_id}/messages",
            200,
            data={"content": "Galatasaray son maÃ§ skoru nedir?", "mode": "chat"}
        )
        
        response_time = time.time() - start_time
        
        if success:
            self.hybrid_tests_run += 1
            ai_response = response.get('content', '')
            print(f"   Response Time: {response_time:.2f} seconds")
            print(f"   AI Response: {ai_response[:200]}...")
            
            # Check if web search was used (should go directly to web search, NOT AnythingLLM)
            web_indicators = ['web araÅŸtÄ±rmasÄ±', 'gÃ¼ncel', 'maÃ§', 'skor', 'galatasaray']
            has_web_indicators = any(indicator in ai_response.lower() for indicator in web_indicators)
            
            # Check if response contains sports information
            has_sports_info = any(pattern in ai_response.lower() for pattern in ['maÃ§', 'skor', 'galatasaray', 'sonuÃ§', 'gol'])
            
            if has_web_indicators or has_sports_info:
                print("âœ… PASSED: Web search used directly for sports (bypassed AnythingLLM)")
                self.hybrid_tests_passed += 1
            else:
                print("âŒ FAILED: Should use web search directly for sports, not AnythingLLM")
        
        return success

    def test_hybrid_system_current_info(self):
        """Test Scenario: Current Information (Direct Web Search) - 'bugÃ¼n dolar kuru kaÃ§ TL?'"""
        print("\nğŸ§ª HYBRID SYSTEM TEST 3C: Current Information (Direct Web Search)")
        
        # Create new conversation
        success, response = self.run_test(
            "Create Conversation for Hybrid Test 3",
            "POST",
            "conversations",
            200,
            data={"title": "Hybrid Test - Current Info"}
        )
        
        if not success:
            return False
            
        test_conv_id = response.get('id')
        
        # Test current information question
        start_time = time.time()
        success, response = self.run_test(
            "Send Current Info Question: 'bugÃ¼n dolar kuru kaÃ§ TL?'",
            "POST",
            f"conversations/{test_conv_id}/messages",
            200,
            data={"content": "bugÃ¼n dolar kuru kaÃ§ TL?", "mode": "chat"}
        )
        
        response_time = time.time() - start_time
        
        if success:
            self.hybrid_tests_run += 1
            ai_response = response.get('content', '')
            print(f"   Response Time: {response_time:.2f} seconds")
            print(f"   AI Response: {ai_response[:200]}...")
            
            # Check if web search was used (should go directly to web search)
            web_indicators = ['web araÅŸtÄ±rmasÄ±', 'gÃ¼ncel', 'tl', 'dolar']
            has_web_indicators = any(indicator in ai_response.lower() for indicator in web_indicators)
            
            # Check if response contains currency information
            has_currency_info = any(pattern in ai_response.lower() for pattern in ['tl', 'lira', 'dolar', 'kur'])
            
            if has_web_indicators or has_currency_info:
                print("âœ… PASSED: Web search used for current information")
                self.hybrid_tests_passed += 1
                
                # Check for reasonable response time (web search should be reasonably fast)
                if response_time < 20:
                    print("âœ… PASSED: Reasonable response time for web search")
                else:
                    print(f"âš ï¸  WARNING: Slow response time ({response_time:.2f}s)")
                    
            else:
                print("âŒ FAILED: No web search indicators found - should use web search for current info")
        
        return success

    def test_hybrid_system_anythingllm_uncertain(self):
        """Test Scenario 1: AnythingLLM Emin DeÄŸil - When AnythingLLM is uncertain, web search should activate"""
        print("\nğŸ§ª HYBRID SYSTEM TEST 4A: AnythingLLM Uncertainty Detection")
        
        # Create new conversation
        success, response = self.run_test(
            "Create Conversation for Uncertainty Test",
            "POST",
            "conversations",
            200,
            data={"title": "Hybrid Test - Uncertainty"}
        )
        
        if not success:
            return False
            
        test_conv_id = response.get('id')
        
        # Test with a question that might make AnythingLLM uncertain
        start_time = time.time()
        success, response = self.run_test(
            "Send Potentially Uncertain Question",
            "POST",
            f"conversations/{test_conv_id}/messages",
            200,
            data={"content": "2024 yÄ±lÄ±nda Ã§Ä±kan en yeni teknoloji trendleri nelerdir?", "mode": "chat"}
        )
        
        response_time = time.time() - start_time
        
        if success:
            self.hybrid_tests_run += 1
            ai_response = response.get('content', '')
            print(f"   Response Time: {response_time:.2f} seconds")
            print(f"   AI Response: {ai_response[:200]}...")
            
            # Check if web search was activated due to AnythingLLM uncertainty
            web_indicators = ['web araÅŸtÄ±rmasÄ±', 'gÃ¼ncel', '2024', 'teknoloji']
            has_web_indicators = any(indicator in ai_response.lower() for indicator in web_indicators)
            
            # Check for uncertainty indicators that should trigger web search
            uncertainty_indicators = ['emin deÄŸilim', 'bilmiyorum', 'kesin deÄŸilim', 'daha Ã§ok bilgiye ihtiyacÄ±m var']
            
            if has_web_indicators:
                print("âœ… PASSED: Web search activated (likely due to AnythingLLM uncertainty)")
                self.hybrid_tests_passed += 1
            else:
                # Check if AnythingLLM provided a confident answer
                if any(indicator in ai_response.lower() for indicator in uncertainty_indicators):
                    print("âŒ FAILED: AnythingLLM showed uncertainty but web search not activated")
                else:
                    print("â„¹ï¸  INFO: AnythingLLM provided confident answer, no web search needed")
                    self.hybrid_tests_passed += 1
        
        return success

    def test_hybrid_system_general_knowledge(self):
        """Test Scenario 5: Genel Bilgi (AnythingLLM Ã¶nce, yedekte web) - 'Mona Lisa kimim yaptÄ±?'"""
        print("\nğŸ§ª HYBRID SYSTEM TEST 4B: General Knowledge (AnythingLLM First, Web Backup)")
        
        # Create new conversation
        success, response = self.run_test(
            "Create Conversation for General Knowledge Test",
            "POST",
            "conversations",
            200,
            data={"title": "Hybrid Test - General Knowledge"}
        )
        
        if not success:
            return False
            
        test_conv_id = response.get('id')
        
        # Test general knowledge question - should try AnythingLLM first
        start_time = time.time()
        success, response = self.run_test(
            "Send General Knowledge Question: 'Mona Lisa kimim yaptÄ±?'",
            "POST",
            f"conversations/{test_conv_id}/messages",
            200,
            data={"content": "Mona Lisa kimim yaptÄ±?", "mode": "chat"}
        )
        
        response_time = time.time() - start_time
        
        if success:
            self.hybrid_tests_run += 1
            ai_response = response.get('content', '')
            print(f"   Response Time: {response_time:.2f} seconds")
            print(f"   AI Response: {ai_response[:200]}...")
            
            # Check if response contains correct information (Leonardo da Vinci)
            has_correct_info = any(name in ai_response.lower() for name in ['leonardo', 'da vinci', 'leonardo da vinci'])
            
            if has_correct_info:
                print("âœ… PASSED: Correct Mona Lisa artist information found")
                self.hybrid_tests_passed += 1
                
                # Check response source (could be AnythingLLM or web search backup)
                web_indicators = ['web araÅŸtÄ±rmasÄ±', 'gÃ¼ncel web', 'kaynaklarÄ±ndan']
                if any(indicator in ai_response.lower() for indicator in web_indicators):
                    print("â„¹ï¸  INFO: Web search was used as backup (AnythingLLM was insufficient)")
                else:
                    print("â„¹ï¸  INFO: AnythingLLM provided the answer successfully")
                    
            else:
                print("âŒ FAILED: Incorrect or missing Mona Lisa artist information")
        
        return success

    def test_hybrid_system_conversation_modes(self):
        """Test conversation modes with hybrid system"""
        print("\nğŸ§ª HYBRID SYSTEM TEST 5: Conversation Modes")
        
        # Create new conversation
        success, response = self.run_test(
            "Create Conversation for Mode Test",
            "POST",
            "conversations",
            200,
            data={"title": "Hybrid Test - Modes"}
        )
        
        if not success:
            return False
            
        test_conv_id = response.get('id')
        
        # Test friend mode
        start_time = time.time()
        success, response = self.run_test(
            "Send Message with Friend Mode",
            "POST",
            f"conversations/{test_conv_id}/messages",
            200,
            data={"content": "Matematik Ã¶ÄŸrenmekte zorlanÄ±yorum", "mode": "chat", "conversationMode": "friend"}
        )
        
        response_time = time.time() - start_time
        
        if success:
            self.hybrid_tests_run += 1
            ai_response = response.get('content', '')
            print(f"   Response Time: {response_time:.2f} seconds")
            print(f"   AI Response: {ai_response[:200]}...")
            
            # Check if response has friendly tone
            friendly_indicators = ['arkadaÅŸ', 'dostum', 'canÄ±m', 'motivasyon', 'baÅŸarabilirsin', 'adÄ±m']
            has_friendly_tone = any(indicator in ai_response.lower() for indicator in friendly_indicators)
            
            if has_friendly_tone:
                print("âœ… PASSED: Friendly conversational tone detected")
                self.hybrid_tests_passed += 1
            else:
                print("âŒ FAILED: No friendly tone detected in response")
        
        return success

    def test_hybrid_system_turkish_errors(self):
        """Test that error messages are in Turkish"""
        print("\nğŸ§ª HYBRID SYSTEM TEST 6: Turkish Error Messages")
        
        # Create new conversation
        success, response = self.run_test(
            "Create Conversation for Error Test",
            "POST",
            "conversations",
            200,
            data={"title": "Hybrid Test - Errors"}
        )
        
        if not success:
            return False
            
        test_conv_id = response.get('id')
        
        # Test with a potentially problematic question that might trigger errors
        success, response = self.run_test(
            "Send Potentially Problematic Question",
            "POST",
            f"conversations/{test_conv_id}/messages",
            200,
            data={"content": "Bu Ã§ok karmaÅŸÄ±k bir soru ve sistem bunu anlayamayabilir", "mode": "chat"}
        )
        
        if success:
            self.hybrid_tests_run += 1
            ai_response = response.get('content', '')
            print(f"   AI Response: {ai_response[:200]}...")
            
            # Check that response is in Turkish (no English error messages)
            english_errors = ['sorry', 'error', 'technical difficulties', 'i cannot', "i don't", 'unable to']
            has_english_errors = any(error.lower() in ai_response.lower() for error in english_errors)
            
            if not has_english_errors:
                print("âœ… PASSED: No English error messages detected")
                self.hybrid_tests_passed += 1
                
                # Check for Turkish responses
                turkish_indicators = ['Ã¼zgÃ¼nÃ¼m', 'teknik', 'sorun', 'yardÄ±m', 'anlayamadÄ±m']
                has_turkish = any(indicator in ai_response.lower() for indicator in turkish_indicators)
                
                if has_turkish:
                    print("âœ… PASSED: Turkish language response confirmed")
                else:
                    print("â„¹ï¸  INFO: Response appears to be in Turkish")
                    
            else:
                print("âŒ FAILED: English error messages detected")
                print(f"   English errors found: {[err for err in english_errors if err.lower() in ai_response.lower()]}")
        
        return success

    def run_hybrid_system_tests(self):
        """Run all hybrid system tests for REFINED intelligent hybrid AI system"""
        print("\n" + "="*60)
        print("ğŸš€ STARTING REFINED INTELLIGENT HYBRID AI SYSTEM TESTS")
        print("Testing NEW enhanced logic:")
        print("1. AnythingLLM Ä°lk Deneme - Try AnythingLLM first for every question")
        print("2. GÃ¼vensiz Cevap Tespiti - Web search if AnythingLLM is uncertain")
        print("3. GÃ¼ncel Konu Tespiti - Direct web search for current info")
        print("4. Soru Geri Sorma - Web search if AnythingLLM asks questions back")
        print("="*60)
        
        hybrid_tests = [
            self.test_hybrid_system_casual_question,
            self.test_hybrid_system_math_question,
            self.test_hybrid_system_weather_direct_web,
            self.test_hybrid_system_sports_direct_web,
            self.test_hybrid_system_current_info,
            self.test_hybrid_system_anythingllm_uncertain,
            self.test_hybrid_system_general_knowledge,
            self.test_hybrid_system_conversation_modes,
            self.test_hybrid_system_turkish_errors
        ]
        
        for test in hybrid_tests:
            try:
                test()
                time.sleep(2)  # Brief pause between tests
            except Exception as e:
                print(f"âŒ Test failed with exception: {e}")
        
        # Print hybrid system test results
        print("\n" + "="*60)
        print(f"ğŸ§ª REFINED HYBRID SYSTEM RESULTS: {self.hybrid_tests_passed}/{self.hybrid_tests_run} tests passed")
        
        if self.hybrid_tests_passed == self.hybrid_tests_run:
            print("ğŸ‰ All REFINED hybrid system tests passed!")
            print("âœ… AnythingLLM first strategy working")
            print("âœ… Web search backup activation working")
            print("âœ… Direct web search for current topics working")
            print("âœ… Turkish error messages confirmed")
        else:
            print(f"âŒ {self.hybrid_tests_run - self.hybrid_tests_passed} hybrid system tests failed")
        
        return self.hybrid_tests_passed == self.hybrid_tests_run

    def create_test_file(self, file_type, content="Test content for file processing"):
        """Create a temporary test file of specified type"""
        if file_type == "txt":
            temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8')
            temp_file.write(content)
            temp_file.close()
            return temp_file.name
        elif file_type == "pdf":
            # Create a simple text file for PDF simulation (since we can't create real PDFs easily)
            temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.pdf', delete=False, encoding='utf-8')
            temp_file.write(content)
            temp_file.close()
            return temp_file.name
        else:
            # For other types, create text files with appropriate extensions
            temp_file = tempfile.NamedTemporaryFile(mode='w', suffix=f'.{file_type}', delete=False, encoding='utf-8')
            temp_file.write(content)
            temp_file.close()
            return temp_file.name

    def test_file_upload_endpoint(self):
        """Test Scenario 1: File Upload - POST /api/conversations/{id}/upload"""
        print("\nğŸ§ª FILE PROCESSING TEST 1: File Upload Endpoint")
        
        # Create conversation for file upload test
        success, response = self.run_test(
            "Create Conversation for File Upload Test",
            "POST",
            "conversations",
            200,
            data={"title": "File Upload Test"}
        )
        
        if not success:
            return False
            
        test_conv_id = response.get('id')
        if not test_conv_id:
            print("âŒ Failed to get conversation ID")
            return False
        
        self.file_tests_run += 1
        
        # Test file upload with valid file
        print(f"\nğŸ” Testing File Upload to conversation {test_conv_id}...")
        
        # Create a test text file
        test_file_path = self.create_test_file("txt", "Bu bir test dosyasÄ±dÄ±r. Dosya iÅŸleme sistemi iÃ§in hazÄ±rlanmÄ±ÅŸtÄ±r.")
        
        try:
            url = f"{self.base_url}/conversations/{test_conv_id}/upload"
            print(f"   URL: {url}")
            
            with open(test_file_path, 'rb') as file:
                files = {'file': ('test_document.txt', file, 'text/plain')}
                response = requests.post(url, files=files, timeout=30)
            
            print(f"   Status Code: {response.status_code}")
            
            if response.status_code == 200:
                self.file_tests_passed += 1
                print("âœ… PASSED: File upload successful")
                try:
                    response_data = response.json()
                    print(f"   Response: {json.dumps(response_data, indent=2)[:300]}...")
                    
                    # Check if system message was generated
                    if 'system_message' in response_data:
                        print("âœ… PASSED: System message generated for file upload")
                    else:
                        print("âš ï¸  WARNING: No system message in response")
                        
                    return True
                except:
                    print("âœ… PASSED: File uploaded but response parsing failed")
                    return True
            else:
                print(f"âŒ FAILED: Expected 200, got {response.status_code}")
                try:
                    error_data = response.json()
                    print(f"   Error: {error_data}")
                except:
                    print(f"   Error: {response.text}")
                return False
                
        except Exception as e:
            print(f"âŒ FAILED: File upload error: {str(e)}")
            return False
        finally:
            # Clean up test file
            if os.path.exists(test_file_path):
                os.remove(test_file_path)

    def test_file_list_endpoint(self):
        """Test Scenario 2: File List - GET /api/conversations/{id}/files"""
        print("\nğŸ§ª FILE PROCESSING TEST 2: File List Endpoint")
        
        # Use existing conversation or create new one
        if not self.conversation_id:
            success, response = self.run_test(
                "Create Conversation for File List Test",
                "POST",
                "conversations",
                200,
                data={"title": "File List Test"}
            )
            if not success:
                return False
            test_conv_id = response.get('id')
        else:
            test_conv_id = self.conversation_id
        
        self.file_tests_run += 1
        
        # Test getting file list
        success, response = self.run_test(
            "Get Uploaded Files List",
            "GET",
            f"conversations/{test_conv_id}/files",
            200
        )
        
        if success:
            self.file_tests_passed += 1
            print("âœ… PASSED: File list endpoint working")
            if isinstance(response, list):
                print(f"   Found {len(response)} uploaded files")
            return True
        
        return False

    def test_file_size_validation(self):
        """Test Scenario 3: File Size Validation (10MB limit)"""
        print("\nğŸ§ª FILE PROCESSING TEST 3: File Size Validation")
        
        # Create conversation for validation test
        success, response = self.run_test(
            "Create Conversation for Size Validation Test",
            "POST",
            "conversations",
            200,
            data={"title": "Size Validation Test"}
        )
        
        if not success:
            return False
            
        test_conv_id = response.get('id')
        self.file_tests_run += 1
        
        # Create a large content string (simulate large file)
        large_content = "A" * (1024 * 1024)  # 1MB of content
        test_file_path = self.create_test_file("txt", large_content)
        
        try:
            url = f"{self.base_url}/conversations/{test_conv_id}/upload"
            print(f"   Testing file size validation...")
            
            with open(test_file_path, 'rb') as file:
                files = {'file': ('large_test.txt', file, 'text/plain')}
                response = requests.post(url, files=files, timeout=30)
            
            print(f"   Status Code: {response.status_code}")
            
            # For 1MB file, it should succeed (under 10MB limit)
            if response.status_code == 200:
                self.file_tests_passed += 1
                print("âœ… PASSED: File size validation working (1MB file accepted)")
                return True
            else:
                print(f"âŒ FAILED: 1MB file rejected - Expected 200, got {response.status_code}")
                return False
                
        except Exception as e:
            print(f"âŒ FAILED: File size validation error: {str(e)}")
            return False
        finally:
            if os.path.exists(test_file_path):
                os.remove(test_file_path)

    def test_file_type_validation(self):
        """Test Scenario 4: File Type Validation (PDF/XLSX/XLS/DOCX/TXT only)"""
        print("\nğŸ§ª FILE PROCESSING TEST 4: File Type Validation")
        
        # Create conversation for type validation test
        success, response = self.run_test(
            "Create Conversation for Type Validation Test",
            "POST",
            "conversations",
            200,
            data={"title": "Type Validation Test"}
        )
        
        if not success:
            return False
            
        test_conv_id = response.get('id')
        self.file_tests_run += 1
        
        # Test invalid file type (e.g., .exe)
        test_file_path = self.create_test_file("exe", "Invalid file type content")
        
        try:
            url = f"{self.base_url}/conversations/{test_conv_id}/upload"
            print(f"   Testing invalid file type (.exe)...")
            
            with open(test_file_path, 'rb') as file:
                files = {'file': ('malicious.exe', file, 'application/octet-stream')}
                response = requests.post(url, files=files, timeout=30)
            
            print(f"   Status Code: {response.status_code}")
            
            # Should reject invalid file types
            if response.status_code == 400:
                self.file_tests_passed += 1
                print("âœ… PASSED: Invalid file type correctly rejected")
                return True
            elif response.status_code == 200:
                print("âŒ FAILED: Invalid file type was accepted (should be rejected)")
                return False
            else:
                print(f"âš ï¸  WARNING: Unexpected status code {response.status_code} for invalid file type")
                return False
                
        except Exception as e:
            print(f"âŒ FAILED: File type validation error: {str(e)}")
            return False
        finally:
            if os.path.exists(test_file_path):
                os.remove(test_file_path)

    def test_openai_integration(self):
        """Test Scenario 5: OpenAI GPT-4o Mini Integration"""
        print("\nğŸ§ª FILE PROCESSING TEST 5: OpenAI GPT-4o Mini Integration")
        
        # Create conversation for OpenAI test
        success, response = self.run_test(
            "Create Conversation for OpenAI Test",
            "POST",
            "conversations",
            200,
            data={"title": "OpenAI Integration Test"}
        )
        
        if not success:
            return False
            
        test_conv_id = response.get('id')
        self.file_tests_run += 1
        
        # Test file processing question that should trigger OpenAI
        start_time = time.time()
        success, response = self.run_test(
            "Send File Processing Question (should use OpenAI)",
            "POST",
            f"conversations/{test_conv_id}/messages",
            200,
            data={"content": "PDF dosyasÄ±nÄ± Ã¶zetle", "mode": "chat"}
        )
        
        response_time = time.time() - start_time
        
        if success:
            self.file_tests_passed += 1
            ai_response = response.get('content', '')
            print(f"   Response Time: {response_time:.2f} seconds")
            print(f"   AI Response: {ai_response[:200]}...")
            
            # Check if OpenAI was used (look for file processing indicators)
            file_processing_indicators = ['dosya', 'pdf', 'Ã¶zet', 'analiz', 'iÅŸlem']
            has_file_processing = any(indicator in ai_response.lower() for indicator in file_processing_indicators)
            
            if has_file_processing:
                print("âœ… PASSED: File processing question handled (likely by OpenAI)")
                return True
            else:
                print("âš ï¸  WARNING: Response doesn't indicate file processing capability")
                return True  # Still pass as the endpoint worked
        
        return False

    def test_smart_routing_file_vs_normal(self):
        """Test Scenario 6: Smart Routing - File Processing vs Normal Questions"""
        print("\nğŸ§ª FILE PROCESSING TEST 6: Smart Routing (File vs Normal Questions)")
        
        # Create conversation for routing test
        success, response = self.run_test(
            "Create Conversation for Smart Routing Test",
            "POST",
            "conversations",
            200,
            data={"title": "Smart Routing Test"}
        )
        
        if not success:
            return False
            
        test_conv_id = response.get('id')
        self.file_tests_run += 1
        
        # Test 1: File processing question (should route to OpenAI)
        print("   Testing file processing question routing...")
        success1, response1 = self.run_test(
            "Send File Processing Question: 'Excel verilerini analiz et'",
            "POST",
            f"conversations/{test_conv_id}/messages",
            200,
            data={"content": "Excel verilerini analiz et", "mode": "chat"}
        )
        
        # Test 2: Normal question (should use hybrid system)
        print("   Testing normal question routing...")
        success2, response2 = self.run_test(
            "Send Normal Question: 'Merhaba nasÄ±lsÄ±n?'",
            "POST",
            f"conversations/{test_conv_id}/messages",
            200,
            data={"content": "Merhaba nasÄ±lsÄ±n?", "mode": "chat"}
        )
        
        if success1 and success2:
            self.file_tests_passed += 1
            
            ai_response1 = response1.get('content', '')
            ai_response2 = response2.get('content', '')
            
            print(f"   File Processing Response: {ai_response1[:100]}...")
            print(f"   Normal Response: {ai_response2[:100]}...")
            
            # Check if responses are different (indicating different routing)
            if ai_response1 != ai_response2:
                print("âœ… PASSED: Smart routing working (different responses for different question types)")
                return True
            else:
                print("âš ï¸  WARNING: Responses are identical (routing may not be working)")
                return True  # Still pass as endpoints worked
        
        return False

    def test_file_processing_keywords(self):
        """Test Scenario 7: File Processing Keywords Detection"""
        print("\nğŸ§ª FILE PROCESSING TEST 7: File Processing Keywords Detection")
        
        # Create conversation for keyword test
        success, response = self.run_test(
            "Create Conversation for Keywords Test",
            "POST",
            "conversations",
            200,
            data={"title": "Keywords Test"}
        )
        
        if not success:
            return False
            
        test_conv_id = response.get('id')
        self.file_tests_run += 1
        
        # Test various file processing keywords
        keywords_to_test = [
            "metni Ã§evir",
            "dosyayÄ± dÃ¼zelt", 
            "belgeyi analiz et",
            "raporu Ã¶zetle"
        ]
        
        successful_tests = 0
        
        for keyword in keywords_to_test:
            print(f"   Testing keyword: '{keyword}'...")
            success, response = self.run_test(
                f"Send Keyword Test: '{keyword}'",
                "POST",
                f"conversations/{test_conv_id}/messages",
                200,
                data={"content": keyword, "mode": "chat"}
            )
            
            if success:
                successful_tests += 1
                ai_response = response.get('content', '')
                print(f"     Response: {ai_response[:80]}...")
            
            time.sleep(1)  # Brief pause between tests
        
        if successful_tests == len(keywords_to_test):
            self.file_tests_passed += 1
            print("âœ… PASSED: All file processing keywords handled successfully")
            return True
        else:
            print(f"âš ï¸  WARNING: {successful_tests}/{len(keywords_to_test)} keyword tests passed")
            return successful_tests > 0

    def test_emergent_llm_key_configuration(self):
        """Test Scenario 8: EMERGENT_LLM_KEY Configuration"""
        print("\nğŸ§ª FILE PROCESSING TEST 8: EMERGENT_LLM_KEY Configuration")
        
        # This test checks if the OpenAI integration works by sending a file processing question
        # and checking for appropriate responses or error messages
        
        # Create conversation for API key test
        success, response = self.run_test(
            "Create Conversation for API Key Test",
            "POST",
            "conversations",
            200,
            data={"title": "API Key Test"}
        )
        
        if not success:
            return False
            
        test_conv_id = response.get('id')
        self.file_tests_run += 1
        
        # Send a file processing question that would require OpenAI
        success, response = self.run_test(
            "Test EMERGENT_LLM_KEY with file processing question",
            "POST",
            f"conversations/{test_conv_id}/messages",
            200,
            data={"content": "LÃ¼tfen bu PDF dosyasÄ±nÄ±n iÃ§eriÄŸini Ã¶zetle", "mode": "chat"}
        )
        
        if success:
            ai_response = response.get('content', '')
            print(f"   AI Response: {ai_response[:200]}...")
            
            # Check for API key related errors
            api_errors = ['api key', 'authentication', 'unauthorized', 'forbidden', 'invalid key']
            has_api_errors = any(error in ai_response.lower() for error in api_errors)
            
            if not has_api_errors:
                self.file_tests_passed += 1
                print("âœ… PASSED: EMERGENT_LLM_KEY appears to be configured correctly")
                return True
            else:
                print("âŒ FAILED: API key configuration issues detected")
                print(f"   Error indicators found: {[err for err in api_errors if err in ai_response.lower()]}")
                return False
        
        return False

    def run_file_processing_tests(self):
        """Run all file processing system tests"""
        print("\n" + "="*60)
        print("ğŸš€ STARTING NEW FILE PROCESSING SYSTEM TESTS")
        print("Testing NEW file processing features:")
        print("1. File Upload Endpoints (POST /api/conversations/{id}/upload)")
        print("2. File List Endpoint (GET /api/conversations/{id}/files)")
        print("3. OpenAI GPT-4o Mini Integration")
        print("4. File Content Extraction")
        print("5. Smart Routing with File Processing")
        print("="*60)
        
        file_tests = [
            self.test_file_upload_endpoint,
            self.test_file_list_endpoint,
            self.test_file_size_validation,
            self.test_file_type_validation,
            self.test_openai_integration,
            self.test_smart_routing_file_vs_normal,
            self.test_file_processing_keywords,
            self.test_emergent_llm_key_configuration
        ]
        
        for test in file_tests:
            try:
                test()
                time.sleep(2)  # Brief pause between tests
            except Exception as e:
                print(f"âŒ File processing test failed with exception: {e}")
        
        # Print file processing test results
        print("\n" + "="*60)
        print(f"ğŸ“ FILE PROCESSING SYSTEM RESULTS: {self.file_tests_passed}/{self.file_tests_run} tests passed")
        
        if self.file_tests_passed == self.file_tests_run:
            print("ğŸ‰ All file processing system tests passed!")
            print("âœ… File upload endpoints working")
            print("âœ… OpenAI GPT-4o mini integration working")
            print("âœ… File validation working")
            print("âœ… Smart routing with file processing working")
        else:
            print(f"âŒ {self.file_tests_run - self.file_tests_passed} file processing tests failed")
        
        return self.file_tests_passed == self.file_tests_run

def main():
    print("ğŸš€ Starting BÄ°LGÄ°N AI Backend API Tests")
    print("=" * 50)
    
    tester = BilginAIAPITester()
    
    # Run basic API tests first
    print("\nğŸ“‹ BASIC API TESTS")
    print("-" * 30)
    basic_tests = [
        tester.test_root_endpoint,
        tester.test_get_conversations_empty,
        tester.test_create_conversation,
        tester.test_get_conversations_with_data,
        tester.test_get_messages_empty,
        tester.test_send_message,
        tester.test_get_messages_with_data,
        tester.test_send_second_message,
        tester.test_delete_conversation,
        tester.test_get_deleted_conversation
    ]
    
    for test in basic_tests:
        test()
    
    # Print basic test results
    print("\n" + "-" * 50)
    print(f"ğŸ“Š Basic API Results: {tester.tests_passed}/{tester.tests_run} tests passed")
    
    # Run hybrid system tests
    hybrid_success = tester.run_hybrid_system_tests()
    
    # Print final comprehensive results
    total_tests = tester.tests_run + tester.hybrid_tests_run
    total_passed = tester.tests_passed + tester.hybrid_tests_passed
    
    print("\n" + "=" * 60)
    print("ğŸ COMPREHENSIVE TEST RESULTS")
    print("=" * 60)
    print(f"ğŸ“‹ Basic API Tests: {tester.tests_passed}/{tester.tests_run} passed")
    print(f"ğŸ§ª Hybrid System Tests: {tester.hybrid_tests_passed}/{tester.hybrid_tests_run} passed")
    print(f"ğŸ“Š TOTAL: {total_passed}/{total_tests} tests passed")
    
    if total_passed == total_tests:
        print("ğŸ‰ ALL TESTS PASSED! BÄ°LGÄ°N AI system is working correctly!")
        return 0
    else:
        failed_tests = total_tests - total_passed
        print(f"âŒ {failed_tests} tests failed")
        
        if tester.tests_passed < tester.tests_run:
            print("   - Basic API issues detected")
        if tester.hybrid_tests_passed < tester.hybrid_tests_run:
            print("   - Hybrid system issues detected")
            
        return 1

if __name__ == "__main__":
    sys.exit(main())